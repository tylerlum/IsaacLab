seed: 42
network:
  separate_value_mlp: False

  mlp:
    units: [512, 512]
  rnn:
    name: lstm
    units: 1024
    layers: 1
    before_mlp: False
    layer_norm: True
    concat_input: False
    concat_output: True

player:
  deterministic: False
  games_num: 100_000
  print_stats: True

ppo:
  multi_gpu: False
  mixed_precision: False
  normalize_input: True
  normalize_value: True
  value_bootstrap: False
  num_actors: -1  # configured from the script (based on num_envs)
  reward_shaper:
    scale_value: 1.0
  normalize_advantage: True
  gamma: 0.998
  tau: 0.95
  learning_rate: 1e-4
  lr_schedule: linear
  schedule_type: standard
  kl_threshold: 0.01
  max_epochs: 200_000
  save_best_after: 100
  save_frequency: 1000
  print_stats: True
  grad_norm: 1.0
  entropy_coef: 0.0
  truncate_grads: True
  e_clip: 0.2
  horizon_length: 16
  minibatch_size: 16384
  mini_epochs: 4
  critic_coef: 4
  clip_value: True
  seq_length: 16
  bound_loss_type: regularisation
  bounds_loss_coef: 0.005
  zero_rnn_on_done: True

  # Choose parameters for asymmetric critic
  # Can't be empty dict, must be None (else gets error looking for keys in this dict)
  asymmetric_critic: ${if:${eval:"'${..asymmetric_critic_name}' == 'mlp'"},${..asymmetric_critic_mlp},${if:${eval:"'${..asymmetric_critic_name}' == 'lstm'"},${..asymmetric_critic_lstm},${eval:None}}}

# Choose asymmetric critic_name in ["mlp", "lstm", <any other string>]
asymmetric_critic_name: ""

asymmetric_critic_mlp:
  name: mlp
  minibatch_size: 16384
  mini_epochs: 4
  learning_rate: 5e-5
  clip_value: True
  normalize_input: True
  truncate_grads: True

  network:
    asymmetric_critic: True
    separate_value_mlp: False

    mlp:
      units: [1024, 512]

asymmetric_critic_lstm:
  name: lstm
  minibatch_size: 16384
  mini_epochs: 4
  learning_rate: 5e-5
  clip_value: True
  normalize_input: True
  truncate_grads: True

  network:
    asymmetric_critic: True
    separate_value_mlp: False

    mlp:
      units: [1024, 512]
    rnn:
      name: lstm
      units: 2048
      layers: 1
      before_mlp: False
      layer_norm: True
      concat_input: False
      concat_output: True